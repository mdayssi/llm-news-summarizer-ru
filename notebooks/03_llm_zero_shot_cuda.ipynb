{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import src.utils.data as data_utils\n",
    "import src.utils.io as io_utils\n",
    "import src.utils.models as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1",
    "outputId": "6a2d2839-d7b9-457e-f7bc-2321a40f7d18"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# EXTERNAL = Path(os.getenv(\"EXTERNAL_STORAGE_DIR\"))\n",
    "ROOT = io_utils.repo_root()\n",
    "SPLIT_DIR = ROOT / \"data/splits\"\n",
    "CONFIG_DIR = ROOT / \"config\"\n",
    "METRIC_DIR = ROOT / \"metrics\"\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2",
    "outputId": "8d75d7f1-2b59-407b-a254-8039a0cacb14"
   },
   "outputs": [],
   "source": [
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "VAL_IDS_PATH = io_utils.load_yaml(CONFIG_DIR / \"dataset.ids.yml\")[\"splits_ids\"][\n",
    "    \"val_ids\"\n",
    "]\n",
    "val_ids = pd.read_csv(ROOT / VAL_IDS_PATH, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566,
     "referenced_widgets": [
      "9d410deda45c436aba17179e09b7599e",
      "e4ccdee1499a4d78a60fa9d05f6ee6cc",
      "d3a0d6ac37294ae58077c7239ab44438",
      "3bcb3852bb9c407aa940f151b41e16ff",
      "1338e9b3c394427a8fce5438a5ce4b93",
      "67c6280af2cb4e0aa8ec5039030629cb",
      "f061778890a343d8a42ddc2d983fed4b",
      "ce33c9fe59ce417badfb9eec5dbafca4",
      "e77848f1cd084209b56e9515808bc4d8",
      "48f12ab7c83143ab91ce5f0c7b696033",
      "f920da0f0a9e4d87b01c6dc1989e3b87",
      "265e30c312f648ddb2fe095d11c5768c",
      "5af491c61d1a4ab2a59a9dd9e7cb7b24",
      "7d80d9b449904cb29d157306f43725f4",
      "bc09c1246ed34a9b89d20311f29fe843",
      "0daf5fee0dcb4f4493d240d37ac2c921",
      "576f4b4606d24159925b80f6ed11fa00",
      "13c40ad246f1402b9433a1cab9ed3ed1",
      "1aa03c4cd63a4da99b650fbbc2204c87",
      "45519b3af35b4f0fa5c0c37849bdf3b4",
      "e97c02a734084758a24772cbd4d7addf",
      "b4c68535d48a42bd860d3bbb234d2f19",
      "45ed6dd37191465a80313d414d96df26",
      "5ef92bf1352045c7bbcc2d8869521061",
      "402d1c72e8fc4e9eb395ddd9b2a96ba9",
      "37a4c228b28245ac9fe119f938c9a739",
      "687a1eeda7144679a79759a91b2ad0d9",
      "90cd512737e94e9597a143ec416262b9",
      "a60e1ba7602a4fb4a729184aa5826213",
      "040c99f5fbc847fab9bc394e4fa899d3",
      "2054e28ff2744691bc621679d03d12bc",
      "208a510a99e841428234e9d2bcdadceb",
      "ba5971bf42a044af8b5c3d9180207a1d",
      "2f55c87b80f0406c9b6c55f816c185fd",
      "5497c5c5c2604fd0b2c159fca1f74c5d",
      "5e7f1f41b55a434c870d847a23a66eb4",
      "019624bd28664f9e8e270051e8b9ac96",
      "0534f970f4b643189ef6a24382430e6f",
      "41c54ea7e21a4f5f92a92c672785c041",
      "3adc3782f250474eb3663fe8965227ab",
      "ccfcc3c7db2e4cb69b4562ed843f75ba",
      "8ea6f6c710e1425d804ee5f2e0410f3e",
      "1b96fec918a546f0908ef69eec17e084",
      "bd96da4ec2654c43935ef275d0c1ff7b",
      "f55c60fe503d45bea1c0e6d51bfe6111",
      "7cadc88ed2d6405fa02d1bac53abce78",
      "14cf2edbc0274d6ebf35a686d8630ed9",
      "6da030bd2b1940ef91a668973e3d9e6c",
      "f0d6bb93afda48edb79fbd68dae956f4",
      "ce048016f1694093b51ad58a6d9e4cd7",
      "9ce28d6d13dc4e02a752206f9be4d4a0",
      "baaf88df1dc244749bfd09585943f4ef",
      "52c4f9009cc1454f837871f71eb1f15a",
      "a8a04971e50e4abfb438f36a0ccc2ad7",
      "8d1ac2e7a6bf49f28d929bc729cd532d",
      "a119000ffdeb45ffb65770bb5490a6ea",
      "92d61916933544c8958b2b951a14770d",
      "79fdde9283b941a7847a17d63ca76ae8",
      "5874eab0bbaa41d0a013974ddba9d432",
      "dfc5b5f358734ccb8617d903b61edbcd",
      "639e7cfad7864d8ab3bac0e34560e179",
      "5698c5e4e7a34de080e773d9b2efa334",
      "c7f7c1e0b7724a28982cab1177096f1c",
      "2422b0e2787b48c891ff98b0a014fd14",
      "a06e7034fd0f4582899d799563b00a65",
      "bf13e3768e0d41e9b022cc7b6abb2639",
      "d06be51d4d0b4e0a854e6a19d7762cfe",
      "8a52438ae21a40d882aad1cd3bd1312e",
      "8aa12803207b4aa0b1c678b8370cdfe5",
      "6f7695d6aef14c6fb362fed9b2911ab5",
      "b444f06350f7448ea174888e5f4026c5",
      "8f675bc5ffdb4588bf54538b7460dc69",
      "d2e5e033ac3242459b56ae071a4b4f02",
      "b33dc8d622aa4912ae45ceb74ed973a5",
      "642138f637044b609da364a9045b3999",
      "b580be4a1e1a4f728a938e0ebf235196",
      "53e1eab9f75f4687a7e020f70067f6c8",
      "38577daf287d46a194368a4379f6b6fe",
      "3b7484f1119448f49196becf0a56cf08",
      "d76762946a004c26ba63ac05105d18fe",
      "a9935a00d1454389a9742a4a22b6e5d8",
      "69368e9b7f8d4c8f80d42fb243cd125d",
      "44830aaf40fc4766a7591ac45383f858",
      "f606db6ec176456384091acd97832e1c",
      "0afbde4b671449939e30715c9b8bc91a",
      "af2fe165440b4406ba50433eab2a2a0c",
      "80e49e22daa04b12be52f8ae6ce318b2",
      "4471a5d7c4c840a7ac76980f71355bda"
     ]
    },
    "id": "4",
    "outputId": "2c94dfad-4474-4761-aaff-87b0532b1bcd"
   },
   "outputs": [],
   "source": [
    "raw_val = load_dataset(\"IlyaGusev/gazeta\")[\"validation\"].to_pandas()\n",
    "\n",
    "print(\"raw val shape:\", raw_val.shape)\n",
    "raw_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "5",
    "outputId": "e015998b-d45e-4726-e51e-fb0d20ccc7b4"
   },
   "outputs": [],
   "source": [
    "val = raw_val.loc[val_ids.squeeze(), [\"title\", \"text\", \"summary\"]]\n",
    "for col in val.columns:\n",
    "    val[col] = data_utils.clean(val[col])\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "0eed3712-732e-4822-c93d-7dc1d20bf30c"
   },
   "outputs": [],
   "source": [
    "MODEL_CFG_PATH = CONFIG_DIR / \"models.params.yml\"\n",
    "model_cfg = None\n",
    "if torch.cuda.is_available():\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cuda_model\"]\n",
    "else:\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cpu_model\"]\n",
    "\n",
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7",
    "outputId": "d9083e8c-750b-4cb4-a508-731f4467558a"
   },
   "outputs": [],
   "source": [
    "device = model_cfg[\"device\"]\n",
    "model_id = model_cfg[\"model_id\"]\n",
    "n_eval = model_cfg[\"n_eval\"]\n",
    "use_4bit = model_cfg[\"use_4bit\"]\n",
    "device_map = model_cfg[\"device_map\"]\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if device == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "if n_eval is None:\n",
    "    subset_val = val\n",
    "else:\n",
    "    subset_val = val.sample(n=min(n_eval, val.shape[0]), random_state=RANDOM_STATE)\n",
    "\n",
    "subset_val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "quantization_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes не готов, продолжаем без 4-бит:\", e)\n",
    "        quantization_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7eb8df94b5e7440ea5d706670d81050c",
      "6e7ec2971b0c434094d0b7009baf4f2a",
      "a3a81937e4dc4d3b8d3586465ea194ed",
      "09cd5b3ecc994c02a7550a030f106763",
      "20ce8fe219994ec283c02c03488834aa",
      "5cbbafef52a14969ab95854c8224563c",
      "931ca0a681144ebca04c843d99de2049",
      "100d8350de304f8eaba71a02f7ef022a",
      "f1d9beeb344b47f194b67a4e4973ecad",
      "f8582c8869734c66b183e384de74690f",
      "62c6a51b326d42218339f909a0ec5ceb"
     ]
    },
    "id": "9",
    "outputId": "0cba8877-d242-4ef2-870a-71e4ae2e9f20"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=(None if quantization_config else torch_dtype),\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# 1) для decoder-only нужно левое выравнивание\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 2) при обрезке важно сохранять «конец» промпта (там префикс ассистента)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "# 3) если у модели нет отдельного PAD — используем EOS\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(model, \"generation_config\", None) is not None:\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if device != \"cuda\":\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "b27e34eaf9ef4992b8d5ea4d1a254fac",
      "e15fe0b9c6594e12a1b88bc32c13f18c",
      "f4e0ab89529741cb809c9290f061e52d",
      "a8a455fe19744be3b861e524ed038382",
      "ee0995e493c345ca9b4520b79f3c75c2",
      "fd6f1745d8e843deb72e08405d3064c2",
      "3da0405e4cde47129fef283391e6ef73",
      "8cde45ba9dcf4ff7b13355ab32432c42",
      "6c1b2ff9c1844533b868f2b32097d581",
      "32ee71202cd24d76a59e0fe9de776f9c",
      "7b06b8f884e44b708f972f4990a2842f"
     ]
    },
    "id": "10",
    "outputId": "fa7e872f-b411-4230-dfb7-ddb185cfb364"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Ты помощник по резюмированию русскоязычных новостей. \"\n",
    "    \"Сделай краткое, нейтральное резюме исходного текста (3–5 предложений). \"\n",
    "    \"Не добавляй фактов, которых нет в тексте.\"\n",
    ")\n",
    "\n",
    "GEN_EVAL = GenerationConfig(\n",
    "    max_new_tokens=160,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# 1) для decoder-only нужно левое выравнивание\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 2) при обрезке важно сохранять «конец» промпта (там префикс ассистента)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "# 3) если у модели нет отдельного PAD — используем EOS\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "MAX_INPUT_TOKENS = model_utils.get_max_input_tokens(tokenizer, GEN_EVAL)\n",
    "\n",
    "\n",
    "def build_chat(text: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Задача: кратко резюмируй.\\n\\nТекст статьи:\\n{text}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    texts: list[str], batch_size: int = 1, show_progress: bool = True\n",
    ") -> list[str]:\n",
    "    out = []\n",
    "    model.eval()\n",
    "\n",
    "    it = range(0, len(texts), batch_size)\n",
    "    if show_progress:\n",
    "        it = tqdm(\n",
    "            it, total=math.ceil(len(texts) / batch_size), desc=\"Generating\", leave=False\n",
    "        )\n",
    "\n",
    "    # на всякий случай — паддинг токен\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for i in it:\n",
    "        chunk = texts[i : i + batch_size]\n",
    "\n",
    "        # 1) шаблон → строки\n",
    "        prompts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                build_chat(t), tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            for t in chunk\n",
    "        ]\n",
    "\n",
    "        # 2) строки → тензоры (BatchEncoding / dict)\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            pad_to_multiple_of=8,\n",
    "            max_length=MAX_INPUT_TOKENS,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=GEN_EVAL,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=getattr(tokenizer, \"eos_token_id\", None),\n",
    "            )\n",
    "\n",
    "        # вырезаем только ответ\n",
    "        gen_ids = output_ids[:, inputs[\"input_ids\"].shape[1] :]\n",
    "        decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        cleaned = [d.strip() for d in decoded]\n",
    "        out.extend(cleaned)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "BATCH = 1 if device != \"cuda\" else 6\n",
    "\n",
    "preds_llm = generate_batch(\n",
    "    subset_val[\"text\"].tolist(), batch_size=BATCH, show_progress=True\n",
    ")\n",
    "refs_llm = subset_val[\"summary\"].tolist()\n",
    "len(preds_llm), len(refs_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11",
    "outputId": "dd5d8d43-7268-4700-8ce7-73c0960f73c0"
   },
   "outputs": [],
   "source": [
    "preds_llm[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12",
    "outputId": "faa16970-62bb-4f1d-ad95-adaf38f13796"
   },
   "outputs": [],
   "source": [
    "refs_llm[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "ec833e58ec014b64b14f9eb53f0ffe1a",
      "9125d72379fd422597ad42e70d30fa9c",
      "a20ee20810244f96a922f14a8d2daa7f",
      "03274e6503324d50a4b18192b9751f90",
      "bfd78424a61841c9b4a0a39cee7a556b",
      "6263f095b6834bcbb7fb27ba2a5951f1",
      "a283eb20f5eb40d793fb27b55232e0ed",
      "26707b96ee474b898087f12ed1addf13",
      "3a8f5b04f3574a88994707dd31ed53a6",
      "28f2066899f04c968ead706a2e5c53c7",
      "0dcc079bd8d748aa8ee6d7b8dca155b8"
     ]
    },
    "id": "13",
    "outputId": "1878611b-ed49-468d-d3ba-22d42224441f"
   },
   "outputs": [],
   "source": [
    "# rouge_scores = data_utils.get_rouge_f1(preds_llm, refs_llm)\n",
    "\n",
    "# rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "WgDrxGKWfVCx"
   },
   "outputs": [],
   "source": [
    "scores = data_utils.get_all_scores(preds_llm, refs_llm, device=device)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "Path(METRIC_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"system\": \"extractive_lead3\",\n",
    "            \"split\": \"validation_full\",\n",
    "            \"rouge1\": scores.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": scores.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": scores.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": scores.get(\"rougeLsum\", 0.0),\n",
    "            \"bertscore_precision\": scores.get(\"bertscore_precision\", 0.0),\n",
    "            \"bertscore_recall\": scores.get(\"bertscore_recall\", 0.0),\n",
    "            \"bertscore_f1\": scores.get(\"bertscore_f1\", 0.0),\n",
    "            \"avg_len_pred\": scores.get(\"avg_len_pred\", 0.0),\n",
    "            \"avg_len_ref\": scores.get(\"avg_len_ref\", 0.0),\n",
    "            \"len_ratio_pred_to_ref\": scores.get(\"len_ratio_pred_to_ref\", 0.0),\n",
    "            \"k\": None,\n",
    "            \"n_examples\": n_eval,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_metrics.to_csv(\n",
    "    METRIC_DIR / f\"llm_zero_shot_validation_{device}_{n_eval}.csv\", index=False\n",
    ")\n",
    "\n",
    "df_sampels = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\": subset_val[\"title\"].head(3) if \"title\" in subset_val else [\"\"] * 3,\n",
    "            \"reference\": refs_llm[:3],\n",
    "            \"prediction\": preds_llm[:3],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_sampels.to_csv(\n",
    "    METRIC_DIR / f\"llm_zero_shot_examples_{device}.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17",
    "outputId": "f85a4adf-6299-4a26-b877-504bf7cac554"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| CUDA доступна:\", torch.cuda.is_available())\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# import os\n",
    "\n",
    "# BASE = \"/content/drive/MyDrive/llm-news\"\n",
    "# for sub in [\"models\", \"metrics\", \"hf_cache\"]:\n",
    "#     os.makedirs(os.path.join(BASE, sub), exist_ok=True)\n",
    "\n",
    "# print(\"Созданы/проверены папки:\", os.listdir(BASE))\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# REPO_URL = \"https://github.com/mdayssi/llm-news-summarizer-ru.git\"\n",
    "# REPO_DIR = \"/content/llm-news\"\n",
    "\n",
    "# if not os.path.exists(REPO_DIR):\n",
    "#     !git clone {REPO_URL} {REPO_DIR}\n",
    "# else:\n",
    "#     print(\"Репозиторий уже есть:\", REPO_DIR)\n",
    "\n",
    "\n",
    "# %cd {REPO_DIR}\n",
    "# !git rev-parse --short HEAD\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# env_path = Path(REPO_DIR) / \".env\"\n",
    "# kv = {\n",
    "#     \"EXTERNAL_MODELS_DIR\": \"/content/drive/MyDrive/llm-news/models\",\n",
    "#     \"EXTERNAL_METRICS_DIR\": \"/content/drive/MyDrive/llm-news/metrics_big\",\n",
    "#     \"EXTERNAL_CACHE_DIR\": \"/content/drive/MyDrive/llm-news/hf_cache\",\n",
    "# }\n",
    "# text = \"\\n\".join([f\"{k}={v}\" for k, v in kv.items()]) + \"\\n\"\n",
    "# env_path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "# print(\".env создано:\")\n",
    "# print(env_path.read_text())\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# %pip -q install --upgrade \\\n",
    "#   evaluate rouge-score bert_score\\\n",
    "#   razdel bitsandbytes\\\n",
    "#   python-dotenv pyyaml \\\n",
    "\n",
    "# import accelerate\n",
    "# import bert_score\n",
    "# import bitsandbytes\n",
    "# import datasets\n",
    "# import dotenv\n",
    "# import evaluate\n",
    "# import razdel\n",
    "# import rouge_score\n",
    "# import sentencepiece\n",
    "# import torch\n",
    "# import tqdm\n",
    "# import transformers\n",
    "# import yaml\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| cuda avail:\", torch.cuda.is_available())\n",
    "# print(\"transformers:\", transformers.__version__)\n",
    "# print(\"datasets:\", datasets.__version__)\n",
    "# print(\"evaluate:\", evaluate.__version__)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# import sys\n",
    "\n",
    "# repo_src = \"/content/llm-news/src\"\n",
    "# if repo_src not in sys.path:\n",
    "#     sys.path.insert(0, repo_src)\n",
    "# print(\"sys.path ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGMliQPofLUy",
    "outputId": "fb068a55-d475-4c47-a4fc-9beebceb41c6"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# !git config --global user.email \"dasha.morgalenko@gmail.com\"\n",
    "# !git config --global user.name \"mdayssi\"\n",
    "# !git remote set-url origin https://$GITHUB_TOKEN@github.com/mdayssi/llm-news-summarizer-ru.git\n",
    "\n",
    "# !git branch --show-current\n",
    "# !git remote -v\n",
    "# !git fetch origin\n",
    "# !git pull --rebase origin main\n",
    "\n",
    "\n",
    "# !git status\n",
    "# !git add .\n",
    "# !git commit -m \"add metrics zero-shot 7B on cuda\"\n",
    "# !git push origin main"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
