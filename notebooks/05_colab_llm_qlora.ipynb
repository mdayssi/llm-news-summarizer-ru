{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import peft\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "import src.utils.data as data_utils\n",
    "import src.utils.io as io_utils\n",
    "import src.utils.models as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# EXTERNAL = Path(os.getenv(\"EXTERNAL_STORAGE_DIR\"))\n",
    "ROOT = io_utils.repo_root()\n",
    "SPLIT_DIR = ROOT / \"data/splits\"\n",
    "CONFIG_DIR = ROOT / \"config\"\n",
    "METRIC_DIR = ROOT / \"metrics\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2",
    "outputId": "2a92380c-920a-4334-dd6a-1ee5631f44e0"
   },
   "outputs": [],
   "source": [
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "IDS_PATH = io_utils.load_yaml(CONFIG_DIR / \"dataset.ids.yml\")[\"splits_ids\"]\n",
    "TRAIN_IDS_PATH = IDS_PATH[\"train_ids\"]\n",
    "VAL_IDS_PATH = IDS_PATH[\"val_ids\"]\n",
    "\n",
    "train_ids = pd.read_csv(ROOT / TRAIN_IDS_PATH, header=None)\n",
    "val_ids = pd.read_csv(ROOT / VAL_IDS_PATH, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566,
     "referenced_widgets": [
      "06709a7049c4445791478d7197670780",
      "0efc5a4325644225904de96a148971d4",
      "30a1fd2cd92145c2b58e607597e667e5",
      "e1e53895a63943af8d7767409bed8547",
      "75e6d215b2b0461b947b119b4a61a2c3",
      "c42a1a4f9d5d49b295f05ce75e73e57d",
      "aa3a81e8367e4e8cafeb5d988d0e9258",
      "77c4aa0ebdf244d0b94d25cbc6187e59",
      "4f2fd1b6dce345d29dc4ea34641eb11f",
      "4247825d086746a79eaf54e53fe6d3b1",
      "a0d4373ca15249199e14820cc1fdb07f",
      "a2de4a76d50e44a883f96181799d6f9a",
      "5f7356b8a6a946eeb62e676c73121328",
      "eb5d10a1c49a4647989926f630230bbc",
      "258762df18d2491a8bb55b626e27ac86",
      "f872e148ea7544f8826fa619abd4e6a7",
      "dee22db194d0402683bdf9a57e1a0aeb",
      "366198fbf04a471794bd72cb4369852f",
      "c15a8d6e433f422186eff21a6e128eee",
      "f51abeb51dd84763acebf5bf65a7fb1b",
      "1e65cec1b7554e4a98ccbb8a04c261e8",
      "e903a2fbc672411686dee97f3692169e",
      "c8ef17e531d14cbeb5003f8d1d961f32",
      "36e6ecadcbb14039a697a9bf3e0f5c99",
      "d7d0b38cdb464ec5b280e277f33bee3c",
      "98935ecc33b74d05b0136a60c61b19aa",
      "fc62f0594d1840d9b068a21ed3130d2e",
      "99bdd4498a86469f83c26ec2dba785bc",
      "0aa6cf009b3e49aa8916c777d2792515",
      "aabd8833062b4efb93b4b00171ed8880",
      "eb2f04671fb543a398f53c62a3a3fb83",
      "8320755b97e44946988127c158b0af54",
      "4ddf785e903f4843bc41a02526aeec61",
      "482ba9fecc244ed6a907b8a44aecc9ca",
      "31368c38e9a14079aba304f0ae0e1f26",
      "308f2547aac34480a61590e4d6d2fa59",
      "b9b1e7e505474cc48550b71eeb96589b",
      "da99fc2381134f79ab963d2776809d96",
      "b00a99acbb564bafb3b99f7b16c5b013",
      "dea791ff2aa84ed8955e4e3e38231971",
      "d4b75324b39541ac9765dbf83830aac7",
      "d99cba5b4f8b4fd6954b24bc3e06e137",
      "65957244c472490ebc1a8eeea77a1ac9",
      "4517eb5f86d54433a8f0f5647d32935b",
      "fce4d6699b2e4a5bb61033359ffb1658",
      "3d307c29d2cc4cfd8418de96ccd4d03f",
      "277c9b0812784c52b3fc1f5b6edec936",
      "bfc12645bffa4c2cb38399bd02b2d5d6",
      "3778ed089fd14878acbda4c07bb5e894",
      "ab79577dd1b04d819dae7152a48b56f8",
      "74631d1741494cb5952d375afc1435d1",
      "e5e9db45aeca48b1a100f31c2a8ad57a",
      "16a4408087264dcea23be0fdfda0e8d2",
      "6d3d5f002ab54b5aaa4192a85160bae6",
      "d6c98d9acbba4574aab989bc5c07ec42",
      "202cfa735d024211b7ccbb4eefd339bb",
      "5dec3d263c0d407283f2ebc4891a1ea8",
      "8d828711e0f84e568612854f8acbf146",
      "154a78f2ff9f4ebe9ce7ffcf2ebe0e25",
      "1f5b019436974d9089f273dab9c6f791",
      "901ac9d7cd4f4aac91c3489cc73bfdf9",
      "732c91c1ad5840f9ae9de27e879d4f44",
      "0e155019741c4c1398c4505e112ada33",
      "cc2dee1ae9b749ff956db9f7676ba442",
      "61ba518851c14865a596a8e0050534e5",
      "0308566f587f4c1cad2c92db8b2fad1a",
      "aa4f6afdfa324fd08e8a50260fc4bdeb",
      "021e302665f14c54a846afec0111d273",
      "3ac98a263ef94328883c309f4e6fc25f",
      "d4e125a573bd44798d59844664aa9368",
      "48d5a086ec06496dadd408efaf30f4be",
      "2891f800f23c4818802fb03ec4b47de3",
      "d0d793ea07b14a4c8ae7252dbbac0379",
      "1a1aff56931e4d0a8cfa311e0099e3d5",
      "55ed828cf1304decbae817daeada3414",
      "1f687c791c344fbe8ff37f2ed11709fb",
      "338536de1d944b84a6a5f65a52015a10",
      "5dc3b6951211432d9a373c920a0f4b00",
      "f566aadca3584c1e98608f5f967a4424",
      "45d5ecd7c81240b4ad9673433bfa7d50",
      "d2c4e70575004da39d470c5b4b0fd612",
      "e1997c3ff9a348748b2e0b4bb727fa5a",
      "6d05057a63134fe397f41a435511d1c8",
      "834f9d3284b946ccbefcd753a4fff0ae",
      "1b2b5c6a4a4d4ce0b4128c4503932c70",
      "65b0b228e26e4a1699c5a07086f64ad0",
      "927532f832b548abbb430abdfbba3dc7",
      "da02192565cd4847a57f2f56bb574525"
     ]
    },
    "id": "4",
    "outputId": "e313d050-9a4a-438c-e1da-d50a0deeb2ea"
   },
   "outputs": [],
   "source": [
    "raw_train = load_dataset(\"IlyaGusev/gazeta\")[\"train\"].to_pandas()\n",
    "raw_val = load_dataset(\"IlyaGusev/gazeta\")[\"validation\"].to_pandas()\n",
    "\n",
    "print(\"raw train shape:\", raw_train.shape, \"raw val shape:\", raw_val.shape)\n",
    "raw_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "5",
    "outputId": "650c68e6-6f53-4644-c054-04b01eb18a9c"
   },
   "outputs": [],
   "source": [
    "columns = [\"text\", \"summary\"]\n",
    "train = raw_train.loc[train_ids.squeeze(), columns]\n",
    "val = raw_val.loc[val_ids.squeeze(), columns]\n",
    "for col in columns:\n",
    "    train[col] = data_utils.clean(train[col])\n",
    "    val[col] = data_utils.clean(val[col])\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "588391e3-5ffd-41d6-c62c-b30e86e210de"
   },
   "outputs": [],
   "source": [
    "MODEL_CFG_PATH = CONFIG_DIR / \"models.params.yml\"\n",
    "model_cfg = None\n",
    "if torch.cuda.is_available():\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cuda_model\"]\n",
    "else:\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cpu_model\"]\n",
    "\n",
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7",
    "outputId": "99ec5020-9065-43ff-a691-8a7ab6a53d9c"
   },
   "outputs": [],
   "source": [
    "device = model_cfg[\"device\"]\n",
    "model_id = model_cfg[\"model_id\"]\n",
    "n_eval = model_cfg[\"n_eval\"]\n",
    "n_train = model_cfg[\"n_train\"]\n",
    "use_4bit = model_cfg[\"use_4bit\"]\n",
    "device_map = model_cfg[\"device_map\"]\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if device == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "subset_val = val.sample(\n",
    "    n=min(n_eval, val.shape[0]), random_state=RANDOM_STATE\n",
    ").reset_index(drop=True)\n",
    "\n",
    "subset_train = train.sample(\n",
    "    n=min(n_train, train.shape[0]), random_state=RANDOM_STATE\n",
    ").reset_index(drop=True)\n",
    "\n",
    "subset_val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "quantization_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes не готов, продолжаем без 4-бит:\", e)\n",
    "        quantization_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "36595d06382c46f5aebcd656a5eaefdf",
      "4aa0a274360c42419058d0bc3a42e033",
      "c4fdef2c4c8c48c889ecec6cbfa39611",
      "4521b1ab29934aaa99ef471eae9bd146",
      "eab5d843c40b4fccb03603104ebc2dc2",
      "ec8e34dbe09a4b94af30a2c3e7b73013",
      "792889950d6640a48ecb3102a5edbb59",
      "13f0570a0e644efd9ffc44896ef9955d",
      "61a7e0171df24920b21e69f3aa989d85",
      "bb0b434a9b1e4a2fb11fec0f54efd730",
      "4095da8288124dadb564bd0e084e54bc"
     ]
    },
    "id": "9",
    "outputId": "e69bc7e6-0fa5-44aa-b88c-4a75b885efa9"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=(None if quantization_config else torch_dtype),\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(model, \"generation_config\", None) is not None:\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if device != \"cuda\":\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Ты помощник по резюмированию русскоязычных новостей. \"\n",
    "    \"Сделай краткое, нейтральное резюме исходного текста (3–5 предложений). \"\n",
    "    \"Не добавляй фактов, которых нет в тексте.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_messages(row):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":\n",
    "                f\"Задача: кратко резюмируй.\\n\\nТекст статьи:\\n{row['text']}\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": row[\"summary\"]},\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480,
     "referenced_widgets": [
      "3360afd54b8e436b94a2a91f7223bd0d",
      "eb946bb9abb24012803850a54a231c4b",
      "66b980028bae464b9740024ec86498b7",
      "baa72b58c2b64b1a8d42c582f741900e",
      "a2574472e7054ef2ba4cc02ed7d14a1f",
      "3fbbbb675d5f4bfda1d36160767440f7",
      "c3e3cb169ebc45f0a6765d445c1d5474",
      "70dd6f7f27db4e1c9ff16a3828559e4a",
      "a5ebee653a7e461f85bfcf3a24aedfad",
      "ddac869d04814384b01b94ed1f2d0902",
      "ce96846fd2b346b5baf588c04d9aa9ad",
      "c811e981fa124463a1fada5a035a1b53",
      "5f002593ed4d415fad5f58a540d7d248",
      "00794a40d3494c8e8c75c08cbb1550b2",
      "7cfe385e7db94237ac756801f4ae1147",
      "359b2868df6e4421a015d911d8fdd81e",
      "1c66baceb6524c3cbc6c13692d034db9",
      "a7c1bb15b07642c997f9fade9d58d364",
      "afa0110c797b4207b8ff68197a89968f",
      "5ed5e401ddaf44549d214f71caacc079",
      "44e6ecf5582f4e3c926c4f266bde86eb",
      "74b006b896b645bb8d852ef05b115c3c"
     ]
    },
    "id": "11",
    "outputId": "1fcec77a-89f1-47f5-bf08-e0cc05202390"
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(subset_train).map(\n",
    "    build_messages, remove_columns=subset_train.columns.to_list()\n",
    ")\n",
    "val_ds = Dataset.from_pandas(subset_val).map(\n",
    "    build_messages, remove_columns=subset_val.columns.to_list()\n",
    ")\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4ydDAB2wC69",
    "outputId": "130e49dd-b6af-432a-fd35-cf0da637032f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    LinearLike = (nn.Linear, bnb.nn.Linear4bit, bnb.nn.Linear8bitLt)\n",
    "except Exception:\n",
    "    LinearLike = (nn.Linear,)\n",
    "\n",
    "suffixes = sorted({n.split(\".\")[-1] for n,m in model.named_modules() if isinstance(m, LinearLike)})\n",
    "print(\"Linear suffixes:\", suffixes)\n",
    "# пример: ['W_pack', 'o_proj', 'w1', 'w2', 'w3']  # для Qwen-1.5, например"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "peft_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'],\n",
    "    modules_to_save=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "CHECK_PATH = str(ROOT / \"src/checkpoints\")\n",
    "\n",
    "if getattr(model, \"is_loaded_in_4bit\", False) or getattr(\n",
    "    model, \"is_loaded_in_8bit\", False\n",
    "):\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    )\n",
    "model = get_peft_model(model, peft_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9M3Uk9b1WFL",
    "outputId": "b7156b44-cfda-476e-9d13-f7efd7e83b0e"
   },
   "outputs": [],
   "source": [
    "print(\"is PeftModel:\", isinstance(model, peft.PeftModel))\n",
    "\n",
    "# 2) Есть ли где-то lora_A/lora_B?\n",
    "wrapped = [n for n, m in model.named_modules() if hasattr(m, \"lora_A\")]\n",
    "print(\"LoRA-wrapped modules (first 20):\", wrapped[:20])\n",
    "\n",
    "# 3) lm_head стал обучаемым (из-за modules_to_save)?\n",
    "print(\"lm_head requires_grad:\", getattr(getattr(model, \"lm_head\", None), \"weight\", None) is not None\n",
    "      and model.lm_head.weight.requires_grad)\n",
    "\n",
    "# 4) Итог\n",
    "model.print_trainable_parameters()\n",
    "assert any(p.requires_grad for p in model.parameters()), \"No trainable parameters!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "b2680d0c5c19491497187134fa855337",
      "8203405f373b4e9e80be9b1d3c8758b2",
      "215f5deff3a24dda8189c6b6f7a9723f",
      "8468c86ba625437ba47491fa16ca63cd",
      "dcf1a6a5038c45f28bb5035aeeeb2c03",
      "18a3233534ee4381b7476abc8a18fb2c",
      "c38179ce48de4d73b18d0163221a5cad",
      "77838a56736b4ee4bc5b6bfcef15360d",
      "2769b9215426447b8c53570f3d69850b",
      "c0eba7727ef04c11a7eb2ba88e1375bd",
      "622c24d9abff4adb93945cef2d24c220",
      "6459b8557c554ce6ac84d364d767f2cc",
      "08c567e58bed49018a959ef0acec69d7",
      "a83b7b40a48f49b3b1e71808c4e7e6b9",
      "e7e42213e8574107b30688a1033048f7",
      "4098eaa0320f425b8035a1c233bba182",
      "692cd7916481423f8947e688010bb08d",
      "c36458d4e7e34ff39bf8855df9770fc0",
      "5c786565f16c4c2ca897cb3cf53bb1e3",
      "5004ca213bc3491f9c71fef29c4925cb",
      "27b4ec653bf143fdabdf1e65312eefd2",
      "86659610b0b245f7b5b5306dcf950525",
      "8e126522f8e54e34a36765eaa5bb4c6e",
      "9cc0459c90ab4a7d9718c77ee41d0af3",
      "a602af8e5a0b4eb0886859658b49b288",
      "bc70845da1bb40fbb1d25aaafaf893d2",
      "6b5d5b7621ce463889d0d7f729e565d4",
      "3302cc30a3d4477c99af254623585e04",
      "b6a9b9e002f74c17a45278a92d187acd",
      "2c1ad6aebc3c4321a41a64293b8cced6",
      "b167966c7a2d4b34813ab7238238e9c1",
      "3bb7f124924a48c882ef3d003f27bcf4",
      "82d2fea3a38048c4bbf0b9bcc329f73a",
      "b2a42fd591a54abfb9a7760f99fb8a00",
      "7cc6f07dc01940bda6c6f70a69906bad",
      "afbdc578593a4b4e9f7b2815a9e75589",
      "2cb84b1b773448198c5ab5ad5c1638ec",
      "f9e3dbc5029646fd97d203cd616c2466",
      "920cdfa8ed284b19ac0497c3a8dea30c",
      "0a2acdd57c5847fa8811a30e2afaf4e4",
      "61d4f6c60a2943ad91e69d4b7c157f92",
      "8e5c1456140d4d34a87102d39b86d07c",
      "95abad39420d4a5cb29b7de6255aa341",
      "2656ba2e90fa4af98e614b151182ff68"
     ]
    },
    "id": "9cpxyBlevh-O",
    "outputId": "69d001b8-8adb-4d12-db3b-cb06c5ef8570"
   },
   "outputs": [],
   "source": [
    "sft_cfg = SFTConfig(\n",
    "    output_dir=CHECK_PATH,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    packing=False,\n",
    "    max_length=2048,\n",
    "    bf16=(device == \"cuda\"),\n",
    "    tf32=(device == \"cuda\"),\n",
    "    optim=(\n",
    "        \"adamw_bnb_8bit\"\n",
    "        if use_4bit\n",
    "        else (\"adamw_torch_fused\" if device == \"cuda\" else \"adamw_torch\")\n",
    "    ),\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    group_by_length=True,\n",
    "    eos_token=tokenizer.eos_token,\n",
    "    dataset_text_field=None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    args=sft_cfg,\n",
    "    peft_config=peft_cfg,\n",
    ")\n",
    "\n",
    "GEN_EVAL = GenerationConfig(\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "MAX_INPUT_TOKENS = model_utils.get_max_input_tokens(tokenizer, GEN_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBWZO_R-tT-u",
    "outputId": "3ec3b207-7f4b-476e-b0d3-aee7466e5bae"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "print(\"trainer.model is PeftModel:\", isinstance(trainer.model, PeftModel))\n",
    "print(\"active adapters:\", getattr(trainer.model, \"active_adapters\", None))\n",
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9gsqHqx4Q00",
    "outputId": "7b792dee-52a3-4c9e-f127-92fbf8cc203d"
   },
   "outputs": [],
   "source": [
    "wrapped = [n for n, m in trainer.model.named_modules() if hasattr(m, \"lora_A\")]\n",
    "print(wrapped[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "13",
    "outputId": "be81392f-a39e-44ba-d4f1-51151c3e5c5f"
   },
   "outputs": [],
   "source": [
    "if hasattr(trainer.model, \"config\"):\n",
    "    trainer.model.config.use_cache = False\n",
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvD4kirgOghy",
    "outputId": "a90c7edd-17e1-4dac-e887-1105171e9201"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(sft_cfg.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "def build_chat(text: str):\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Задача: кратко резюмируй.\\n\\nТекст статьи:\\n{text}\",\n",
    "        },\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": [
    "def generate_batch(texts, batch_size=4, show_progress=True):\n",
    "    out = []\n",
    "    it = range(0, len(texts), batch_size)\n",
    "    if show_progress:\n",
    "        it = tqdm(\n",
    "            it,\n",
    "            total=math.ceil(len(texts) / batch_size),\n",
    "            desc=\"Generating SFT infer\",\n",
    "            leave=False,\n",
    "        )\n",
    "\n",
    "    for i in it:\n",
    "        chunk = [build_chat(t) for t in texts[i : i + batch_size]]\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            pad_to_multiple_of=8,\n",
    "            max_length=MAX_INPUT_TOKENS,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, generation_config=GEN_EVAL)\n",
    "\n",
    "        gen_ids = output_ids[:, inputs[\"input_ids\"].shape[1] :]\n",
    "        decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        out.extend([d.strip() for d in decoded])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2kCOc8qjLhr",
    "outputId": "4ff7d113-ea86-4b09-9496-698b940be3d3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_for_inference_with_aligned_head(model):\n",
    "    # 1) инференс-режим: выключаем GC, включаем кэш\n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.use_cache = True\n",
    "    model.eval()\n",
    "\n",
    "    # 2) выбираем \"референсный\" dtype по финальной норме (ближе всего к hidden states)\n",
    "    target_dtype = None\n",
    "    # Qwen2: base_model.model.norm.weight\n",
    "    try:\n",
    "        target_dtype = model.base_model.model.norm.weight.dtype\n",
    "    except Exception:\n",
    "        pass\n",
    "    if target_dtype is None:\n",
    "        # универсальный поиск: какая-нибудь финальная нормализация\n",
    "        for n, p in model.named_parameters():\n",
    "            if n.endswith((\"norm.weight\", \"final_layernorm.weight\", \"ln_f.weight\")):\n",
    "                target_dtype = p.dtype\n",
    "                break\n",
    "    if target_dtype is None:\n",
    "        # запасной вариант\n",
    "        target_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # 3) приводим lm_head к target_dtype\n",
    "    if getattr(model, \"lm_head\", None) is not None:\n",
    "        model.lm_head.to(target_dtype)\n",
    "\n",
    "    # 4) приводим все ModulesToSaveWrapper к target_dtype (совместимо с peft 0.17)\n",
    "    try:\n",
    "        from peft.tuners.tuners_utils import ModulesToSaveWrapper\n",
    "    except Exception:\n",
    "        ModulesToSaveWrapper = tuple()  # на всякий случай\n",
    "\n",
    "    for _, m in model.named_modules():\n",
    "        if isinstance(m, ModulesToSaveWrapper):\n",
    "            try:\n",
    "                m.to(target_dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return target_dtype\n",
    "\n",
    "target_dtype = prepare_for_inference_with_aligned_head(model)\n",
    "print(\"Aligned target dtype:\", target_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "8693bddaf09d4d8889953cf90c02fc7d",
      "60f8bb9c44e34c1d854b06592182740a",
      "00c10708d1fa4d1784a484bc09558e12",
      "e6b890320d6c48c8965c652f123e2a16",
      "9a4fc22183f2467a83f9f51af23735e0",
      "45263ed39ff049eab2b771fa9260215f",
      "5442f51487504d30b24d0f8cee33c11b",
      "28d2624db67d4094a3a390ff397d9a2b",
      "5b8502a1ac6341d0ad326d1f9c33042f",
      "5fdb57e6a622470d862f801b4bb8f88e",
      "5250a3cb1767457eb6af4984bee2cfed"
     ]
    },
    "id": "16",
    "outputId": "3328fa25-f06f-4fc2-80fc-03fe76308357"
   },
   "outputs": [],
   "source": [
    "BATCH = 1 if device != \"cuda\" else 6\n",
    "texts = subset_val[\"text\"].tolist()\n",
    "refs = subset_val[\"summary\"].tolist()\n",
    "preds_sft = generate_batch(texts, batch_size=BATCH, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17",
    "outputId": "6ece0c8e-8022-44cf-b620-ed349537fac3"
   },
   "outputs": [],
   "source": [
    "preds_sft[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18",
    "outputId": "da1027c1-6533-4b3b-ae3c-658ecd5aec4b"
   },
   "outputs": [],
   "source": [
    "refs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415,
     "referenced_widgets": [
      "85c3a9fe96d542619039c989532e8d33",
      "2a8af251222b42a2a6c9ceffe29d2f05",
      "4148de7b9c7a4e7da52448f95137adb3",
      "cf51608892cd4c2c99d7408656e2af9d",
      "d37ecd3e87ad42d3be66e2158b88c6dc",
      "7127770fa7e6402483b522e52827015b",
      "4c236479149d4ad9aa77fc5dd394c03e",
      "4055b55140244342820902a5d9919a00",
      "0b5ad738f1a34c4f8ca27456b9d67563",
      "14d0b1f1476a483b9d9b133baeb60b5a",
      "d6ceeaeb24f2497ab5e4a6a07024411c",
      "5b8dc688034b4d4b96dee8fcdc840fd0",
      "714f282f1b854db6a49f6fa888923312",
      "e5ea115029054249bb70d5200ce89d7c",
      "274c16aaf0104aebb71ef5ff78c5ff8f",
      "5d35d5fdec784d4988b226073b8df4c7",
      "9d01a1fc8edc4f7cad5b932a87579965",
      "d1a44a0f40124370af8a263e23ad87fb",
      "aff712693c8b4625a048ad585e793fa1",
      "e3b7bc752737477e81e30263d13c5583",
      "6082b202b09940f0a69dfe59df50a1e6",
      "6e8b1460fcf04c90bca15cad15179599",
      "aa8f8a3361134664bee91fed5ab9157a",
      "f5988bb5be214be5ac55e5b4c8d58885",
      "523c10fae4724e83b58de64bd3b7b541",
      "9d776d0385e340c0a091a3d2df1247b6",
      "681c7a5636914febb0426be1313d2ca5",
      "0d7eadb3190b43f0a01b7a42edcfd9ec",
      "87821f0b969048f5ae298fc0596a5a9e",
      "6364f5b7e0444cf0a41da86536648b75",
      "7f4664337e4b4f0d84b01301d1fd8322",
      "f62f06a7709d4313bfd4893477cde812",
      "489a86d8dcd84f17874f563f061b85f3",
      "d1109f441ea24aaf9812599b5c5ed9a5",
      "89e2787715b7423189d55fcb3c4b32a3",
      "abb81f61d9a247df9be01fd326a39850",
      "7949c0de635341d39037d1147de5bec2",
      "bf73b51e387343fe9b35a99b96bd86e3",
      "ec4e42cfed9041fca58722fa6aef2d40",
      "67a6738f2c3e49c2a3dac5709bfa41e1",
      "58c5aba2c44d462ba9ad55cd8e6d3a19",
      "ca0ba51762cc4f62bf15cbdf985ea9b4",
      "5d9f6f8a7ae84469b018c09b6a1dc1b4",
      "c4913813360e4371af741ae3e9b8842a",
      "c22e5498f69b41ed93331a4a7c55e6c6",
      "239f2abb63c1442fb9908e98824f7337",
      "14c446ea92d2453381a310b4f0f6e789",
      "8cb89de0a3144101a20d1e967b17f716",
      "4e28db3fb4104950b2ed97ff29c4af50",
      "d2740a215fbf45f1824ba8605062ea2c",
      "9b1b2039deed47448e2d1a7f8aa30061",
      "ad1d683edc6d4b6a803900c7e893def5",
      "2a0b5e4236594edfb0cae868fe78fe06",
      "7f06cfcc77774e168938167d1134a366",
      "842412537bec41318721ff58c8daeb90",
      "2520fd2ed1d2464f845d6dafbed45356",
      "e7b430f40fb74b3a843817960d96a7cd",
      "79a23906e57d4fcea6c2e3aa01446486",
      "94dfd1fcb9d842378de1e62444e108f5",
      "aadaa17a82704e9395702852070af26e",
      "3c6ba5b9025e4117b734504043fd62a3",
      "1572bc7b4e5846858a325d45e44b8186",
      "cbfd70edc3994949aa30eaa23966fc73",
      "1cb307a24aeb45f598c6a5a0e65fc106",
      "d7cf1f6cceca435e9a12a9a40e9c3223",
      "072f048a15e444f9952ac03c00b95b04",
      "a25ab9be3e234282a33190d46fb07eb1",
      "33d366b405fd4d4587624f318e2719c8",
      "46e7998e6dd0454c8271cacce7e9c0cd",
      "042f0e4f408646b89df4faab30fb87cc",
      "a16f7106284948c7bf97ebd696d82c5b",
      "6d9eaa3e297148f89a656c3ad53159f9",
      "580b42a29139466e934d8ecbe06814b9",
      "e78a4ba40a9d47038ff95fdf1371be5e",
      "61cb6a95f08b410a9612eedd15a27577",
      "cf00e33237634cf9ba748b07cf7b8199",
      "4d89555070bb47ffadabc4f8ace9764e"
     ]
    },
    "id": "19",
    "outputId": "a9e7af15-a296-4562-d528-25e90138e1db"
   },
   "outputs": [],
   "source": [
    "scores = data_utils.get_all_scores(preds_sft, refs, device=device)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "20"
   },
   "outputs": [],
   "source": [
    "Path(METRIC_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"system\": \"SRT QLoRA\",\n",
    "            \"split\": \"validation_full\",\n",
    "            \"rouge1\": scores.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": scores.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": scores.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": scores.get(\"rougeLsum\", 0.0),\n",
    "            \"bertscore_precision\": scores.get(\"bertscore_precision\", 0.0),\n",
    "            \"bertscore_recall\": scores.get(\"bertscore_recall\", 0.0),\n",
    "            \"bertscore_f1\": scores.get(\"bertscore_f1\", 0.0),\n",
    "            \"avg_len_pred\": scores.get(\"avg_len_pred\", 0.0),\n",
    "            \"avg_len_ref\": scores.get(\"avg_len_ref\", 0.0),\n",
    "            \"len_ratio_pred_to_ref\": scores.get(\"len_ratio_pred_to_ref\", 0.0),\n",
    "            \"k\": None,\n",
    "            \"n_examples\": n_eval,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_metrics.to_csv(\n",
    "    METRIC_DIR / f\"llm_qlora_validation_{device}_{n_eval}.csv\", index=False\n",
    ")\n",
    "\n",
    "df_sampels = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\": subset_val[\"title\"].head(3) if \"title\" in subset_val else [\"\"] * 3,\n",
    "            \"reference\": refs[:3],\n",
    "            \"prediction\": preds_sft[:3],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_sampels.to_csv(\n",
    "    METRIC_DIR / f\"llm_qlora_examples_{device}.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "21"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "22"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23",
    "outputId": "d149670b-55f0-4655-9192-5a926bcc3013"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"| CUDA доступна:\", torch.cuda.is_available())\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/llm-news\"\n",
    "for sub in [\"models\", \"metrics\", \"hf_cache\"]:\n",
    "    os.makedirs(os.path.join(BASE, sub), exist_ok=True)\n",
    "\n",
    "print(\"Созданы/проверены папки:\", os.listdir(BASE))\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "REPO_URL = \"https://github.com/mdayssi/llm-news-summarizer-ru.git\"\n",
    "REPO_DIR = \"/content/llm-news\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(\"Репозиторий уже есть:\", REPO_DIR)\n",
    "\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "!git rev-parse --short HEAD\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(REPO_DIR) / \".env\"\n",
    "kv = {\n",
    "    \"EXTERNAL_MODELS_DIR\": \"/content/drive/MyDrive/llm-news/models\",\n",
    "    \"EXTERNAL_METRICS_DIR\": \"/content/drive/MyDrive/llm-news/metrics_big\",\n",
    "    \"EXTERNAL_CACHE_DIR\": \"/content/drive/MyDrive/llm-news/hf_cache\",\n",
    "}\n",
    "text = \"\\n\".join([f\"{k}={v}\" for k, v in kv.items()]) + \"\\n\"\n",
    "env_path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "print(\".env создано:\")\n",
    "print(env_path.read_text())\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "%pip -q install --upgrade \\\n",
    "  evaluate rouge-score bert_score\\\n",
    "  razdel bitsandbytes accelerate\\\n",
    "  python-dotenv pyyaml peft trl\n",
    "\n",
    "import accelerate\n",
    "import bert_score\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import dotenv\n",
    "import evaluate\n",
    "import razdel\n",
    "import rouge_score\n",
    "import sentencepiece\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "import yaml\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"| cuda avail:\", torch.cuda.is_available())\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"evaluate:\", evaluate.__version__)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "repo_src = \"/content/llm-news/src\"\n",
    "if repo_src not in sys.path:\n",
    "    sys.path.insert(0, repo_src)\n",
    "print(\"sys.path ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vX-zEjxelDca",
    "outputId": "84664e7d-a55f-4360-81c3-f56b55057beb"
   },
   "outputs": [],
   "source": [
    "!cp /content/llm-news/src/checkpoints /content/drive/MyDrive/llm-news/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "-thnb_WBiy5m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
