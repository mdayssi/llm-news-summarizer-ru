{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0",
        "outputId": "98f8be41-43c1-44e3-f186-07db3a4e89a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 16 15:03:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             42W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "torch: 2.6.0+cu124 | CUDA доступна: True\n"
          ]
        }
      ],
      "source": [
        "# Проверка, что среда Colab активна и доступен GPU\n",
        "!nvidia-smi\n",
        "import torch\n",
        "\n",
        "print(\"torch:\", torch.__version__, \"| CUDA доступна:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1",
        "outputId": "26e3bff5-0dbc-43d0-ac2b-828391572209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "8b7e4702-40ad-44fc-9bb7-32ca88a27ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Созданы/проверены папки: ['models', 'metrics', 'hf_cache', 'data', 'indexes', 'cache']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/llm-news\"\n",
        "for sub in [\"models\", \"metrics\", \"hf_cache\"]:\n",
        "    os.makedirs(os.path.join(BASE, sub), exist_ok=True)\n",
        "\n",
        "print(\"Созданы/проверены папки:\", os.listdir(BASE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3",
        "outputId": "7a289c41-e7dc-4d5c-8322-d0b1dbbed260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Репозиторий уже есть: /content/llm-news\n",
            "/content/llm-news\n",
            "8d2efb0\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "REPO_URL = \"https://github.com/mdayssi/llm-news-summarizer-ru.git\"\n",
        "REPO_DIR = \"/content/llm-news\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "else:\n",
        "    print(\"Репозиторий уже есть:\", REPO_DIR)\n",
        "\n",
        "# Дальше будем работать из корня репо\n",
        "%cd {REPO_DIR}\n",
        "!git rev-parse --short HEAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4",
        "outputId": "4f3c437d-39c9-43e0-e1b6-118ca1697f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env создано:\n",
            "EXTERNAL_MODELS_DIR=/content/drive/MyDrive/llm-news/models\n",
            "EXTERNAL_METRICS_DIR=/content/drive/MyDrive/llm-news/metrics_big\n",
            "EXTERNAL_CACHE_DIR=/content/drive/MyDrive/llm-news/hf_cache\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "env_path = Path(REPO_DIR) / \".env\"\n",
        "kv = {\n",
        "    \"EXTERNAL_MODELS_DIR\": \"/content/drive/MyDrive/llm-news/models\",\n",
        "    \"EXTERNAL_METRICS_DIR\": \"/content/drive/MyDrive/llm-news/metrics_big\",\n",
        "    \"EXTERNAL_CACHE_DIR\": \"/content/drive/MyDrive/llm-news/hf_cache\",\n",
        "}\n",
        "text = \"\\n\".join([f\"{k}={v}\" for k, v in kv.items()]) + \"\\n\"\n",
        "env_path.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "print(\".env создано:\")\n",
        "print(env_path.read_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5",
        "outputId": "3eada0c4-a34c-442b-c776-771679d99c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.6.0+cu124 | cuda avail: True\n",
            "transformers: 4.55.1\n",
            "datasets: 4.0.0\n",
            "evaluate: 0.4.5\n"
          ]
        }
      ],
      "source": [
        "%pip -q install --upgrade \\\n",
        "  evaluate rouge-score \\\n",
        "  razdel bitsandbytes\\\n",
        "  python-dotenv pyyaml \\\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "import rouge_score\n",
        "import evaluate\n",
        "import sentencepiece\n",
        "import razdel\n",
        "import dotenv\n",
        "import yaml\n",
        "import tqdm\n",
        "import bitsandbytes\n",
        "\n",
        "print(\"torch:\", torch.__version__, \"| cuda avail:\", torch.cuda.is_available())\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"evaluate:\", evaluate.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6",
        "outputId": "ae632953-ce38-430b-dc86-b5632267d666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sys.path ok\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "repo_src = \"/content/llm-news/src\"\n",
        "if repo_src not in sys.path:\n",
        "    sys.path.insert(0, repo_src)\n",
        "print(\"sys.path ok\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2+2"
      ],
      "metadata": {
        "id": "t5pidgXhaMOW",
        "outputId": "fdb43489-92b0-4153-ce0c-4e93b511911d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t5pidgXhaMOW",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALXLD68KabBL"
      },
      "id": "ALXLD68KabBL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}