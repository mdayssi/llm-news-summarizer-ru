{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "from src.utils import data as data_utils\n",
    "from src.utils import io as io_utils\n",
    "from src.utils import models as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# EXTERNAL = Path(os.getenv(\"EXTERNAL_STORAGE_DIR\"))\n",
    "ROOT = io_utils.repo_root()\n",
    "SPLIT_DIR = ROOT / \"data/splits\"\n",
    "DPO_DATA = ROOT / \"data/dpo/exp_1\"\n",
    "CONFIG_DIR = ROOT / \"config\"\n",
    "METRIC_DIR = ROOT / \"metrics\"\n",
    "SFT_MODEL_DIR = ROOT / \"models/sft_qlora\"\n",
    "RANDOM_STATE = 42\n",
    "N_DPO = 2000\n",
    "N_EVAL = 800\n",
    "\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2",
    "outputId": "392b91fd-4602-4572-8b3c-04f9fabf2f88"
   },
   "outputs": [],
   "source": [
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "IDS_PATH = io_utils.load_yaml(CONFIG_DIR / \"dataset.ids.yml\")[\"splits_ids\"]\n",
    "TRAIN_IDS_PATH = IDS_PATH[\"train_ids\"]\n",
    "VAL_IDS_PATH = IDS_PATH[\"val_ids\"]\n",
    "\n",
    "train_ids = pd.read_csv(ROOT / TRAIN_IDS_PATH, header=None)\n",
    "val_ids = pd.read_csv(ROOT / VAL_IDS_PATH, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "4",
    "outputId": "34300277-b08a-4737-fe9d-1864547de1f8"
   },
   "outputs": [],
   "source": [
    "raw_train = load_dataset(\"IlyaGusev/gazeta\")[\"train\"].to_pandas()\n",
    "raw_val = load_dataset(\"IlyaGusev/gazeta\")[\"validation\"].to_pandas()\n",
    "\n",
    "print(\"raw train shape:\", raw_train.shape, \"raw val shape:\", raw_val.shape)\n",
    "raw_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "5",
    "outputId": "52a4118a-dba5-4ae5-924b-a812813aeea1"
   },
   "outputs": [],
   "source": [
    "columns = [\"text\", \"summary\"]\n",
    "train = raw_train.loc[train_ids.squeeze(), columns]\n",
    "val = raw_val.loc[val_ids.squeeze(), columns]\n",
    "for col in columns:\n",
    "    train[col] = data_utils.clean(train[col])\n",
    "    val[col] = data_utils.clean(val[col])\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "8f75760e-eb91-4f9e-8ed3-a5f948729561"
   },
   "outputs": [],
   "source": [
    "MODEL_CFG_PATH = CONFIG_DIR / \"models.params.yml\"\n",
    "model_cfg = None\n",
    "if torch.cuda.is_available():\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cuda_model\"]\n",
    "else:\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cpu_model\"]\n",
    "\n",
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7",
    "outputId": "56cee17b-4ee7-41aa-e46f-77777cb521ab"
   },
   "outputs": [],
   "source": [
    "device = model_cfg[\"device\"]\n",
    "model_id = model_cfg[\"model_id\"]\n",
    "use_4bit = model_cfg[\"use_4bit\"]\n",
    "device_map = model_cfg[\"device_map\"]\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if device == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "subset_val = val.sample(\n",
    "    n=min(N_EVAL, val.shape[0]), random_state=RANDOM_STATE\n",
    ").reset_index(drop=True)\n",
    "\n",
    "subset_dpo = train.sample(\n",
    "    n=min(N_DPO, train.shape[0]), random_state=RANDOM_STATE\n",
    ").reset_index(drop=True)\n",
    "\n",
    "subset_val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "quantization_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes не готов, продолжаем без 4-бит:\", e)\n",
    "        quantization_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f68c4f24647c49989e7c235c59b8de03",
      "aa30f9ae91224d43a0fec4e13fe9e837",
      "5198606259674f0e8baa20e6aa9d4df7",
      "b9e2b52ef2b74414bef82e8bf333ab96",
      "3dac969b82f84c169b5e4a5a7d71c2cf",
      "0a95bc64cf594cd8b02224982953d474",
      "1cc05a03b37b433c9ab989ce1e66f825",
      "08f2087f57054efbbd99fad63eca8023",
      "a66c2d35b8994a7e815905758b8ebc8f",
      "2fffc2391e6a40b2bc8698b03491709c",
      "8d62cdafc5dd45ca920a20d01ca1987f"
     ]
    },
    "id": "9",
    "outputId": "6e02cc7c-fa2c-43bc-c9e0-a7097a279e70"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(\n",
    "    base_model,\n",
    "    use_gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "policy_model = PeftModel.from_pretrained(base_model, SFT_MODEL_DIR, is_trainable=True)\n",
    "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(policy_model, \"generation_config\", None) is not None:\n",
    "    policy_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    policy_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "ref_model = copy.deepcopy(policy_model)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "ref_model.eval()\n",
    "\n",
    "device = next(policy_model.parameters()).device\n",
    "policy_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Ты помощник по резюмированию русскоязычных новостей. \"\n",
    "    \"Сделай краткое, нейтральное резюме исходного текста (3–5 предложений). \"\n",
    "    \"Не добавляй фактов, которых нет в тексте.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_chat(text: str) -> str:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Задача: кратко резюмируй.\\n\\nТекст статьи:\\n{text}\",\n",
    "        },\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "GEN_GREEDY = GenerationConfig(\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "GEN_SAMPLED = GenerationConfig(\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "MAX_INPUT_TOKENS = model_utils.get_max_input_tokens(tokenizer, GEN_GREEDY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "def generate_with_cfg(texts, model, gen_cfg, batch_size=4):\n",
    "    out = []\n",
    "    it = tqdm(\n",
    "        range(0, len(texts), batch_size),\n",
    "        total=math.ceil(len(texts) / batch_size),\n",
    "        desc=f\"Generating (do_sample={gen_cfg.do_sample})\",\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for i in it:\n",
    "        chunk = [build_chat(t) for t in texts[i : i + batch_size]]\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            pad_to_multiple_of=8,\n",
    "            max_length=MAX_INPUT_TOKENS,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, generation_config=gen_cfg)\n",
    "\n",
    "        gen_ids = output_ids[:, inputs[\"input_ids\"].shape[1] :]\n",
    "        decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        out.extend([d.strip() for d in decoded])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "ROjkjY720Q8N"
   },
   "outputs": [],
   "source": [
    "del ref_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "policy_model.eval()\n",
    "policy_model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "db19bc7b56a74bcba0c5600c151389f7",
      "a0f80513348e4d798a897976353a2a61",
      "a613f749dc4b41509cc869dba49ad3e7",
      "ae7512a7b109401b9bf4f214a9d40e1c",
      "ec44a4a7223a4df4a2c153b3d1abb89b",
      "65a59836d42d4522811dd2138dcb0fc7",
      "cd2ba6bcef34421485d0bccd31584a6f",
      "fd2a12ff445847059dc55bfd7c39cf0d",
      "5fdb7cce48cc48c8bff5e20a8a5a9e7a",
      "605c0f40687c41d988f8223abb59a158",
      "822394ef442742feb555550ae1783562",
      "238b66f7125b4b94ad657621d4a8e6c3",
      "5869be9d1a554e4e9cf35d69c68f1250",
      "dc1faaf0f8ab47daae916e2c78bec251",
      "5748e553d8b249fbae8fcfc7b51c2882",
      "13153d800735454eb9249607c0044a44",
      "97c568a097774443a7f69aa65af162b1",
      "1fa2e9927e52427781fd9f5a2897e7ad",
      "a2d8253f546d4f02a0185d6244392b2e",
      "5f2f9640f5c4479dbff4ed85848b199e",
      "1af54505fc374d5fb36876fbb4bcb619",
      "6548b15edb9f4473b0b62ea1fb477dad"
     ]
    },
    "id": "13",
    "outputId": "270c63c2-8095-4514-a9e4-26f9cdf07371"
   },
   "outputs": [],
   "source": [
    "BATCH = 6\n",
    "texts_dpo = subset_dpo[\"text\"].tolist()\n",
    "refs_dpo = subset_dpo[\"summary\"].tolist()\n",
    "\n",
    "cand_greedy = generate_with_cfg(texts_dpo, policy_model, GEN_GREEDY, batch_size=BATCH)\n",
    "cand_sampled = generate_with_cfg(texts_dpo, policy_model, GEN_SAMPLED, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "29f203cdeaab451e9421cbcac3d09923",
      "f452bef6e26548a5aaf4e02198e0e00f",
      "13c2d0e5c57849ac8093b648f8828e69",
      "a69f8e526fd449188f02f240980d1399",
      "a80c60c0255b48c3921ff7fed191791b",
      "606f9c45e19b4f0e947823f313acfb2d",
      "559c1e39fe0f486c826072479ac4209c",
      "6c9f070e74884f16b4de59648c924c84",
      "3e8229276f364917b29b5f0dc8c45c6e",
      "261d2d3325894b71b3648a18a932b4a1",
      "73ba9fc124724791b259cfda3082fcc7"
     ]
    },
    "id": "14",
    "outputId": "ab5ce049-a3e0-480e-93ee-602cd1d140e1"
   },
   "outputs": [],
   "source": [
    "rouge_greedy = data_utils.dpo_rouge_lsum(cand_greedy, refs_dpo)\n",
    "rouge_sampled = data_utils.dpo_rouge_lsum(cand_sampled, refs_dpo)\n",
    "\n",
    "pairs = []\n",
    "MIN_DELTA = 0.02\n",
    "for i in range(subset_dpo.shape[0]):\n",
    "    if rouge_greedy[i] > rouge_sampled[i]:\n",
    "        chosen, rejected = cand_greedy[i], cand_sampled[i]\n",
    "        delta = rouge_greedy[i] - rouge_sampled[i]\n",
    "    else:\n",
    "        chosen, rejected = cand_sampled[i], cand_greedy[i]\n",
    "        delta = rouge_sampled[i] - rouge_greedy[i]\n",
    "    if delta >= MIN_DELTA:\n",
    "        pairs.append((build_chat(texts_dpo[i]), chosen, rejected))\n",
    "\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743,
     "referenced_widgets": [
      "c0b1b06974cf4f87b60963a55eff8917",
      "c06fe07780f74d65a323895de377ec83",
      "c8494841e334447a9285a57414ea72df",
      "fe65ab1f7ebd4da2ad7bea849f214dae",
      "93b8a2992c8f4a9abd38ffc44f257c11",
      "5289777b16b1477ead9fd8d05eea89e4",
      "2f08747174fb43b0b48223e5401c6135",
      "654f5737be1c4a6c8a3390daa4296eff",
      "1a7e2fee3b174036844fad6f9644d301",
      "db464a9d771242a8afd156321aef9970",
      "c8c9c8a1fcf1407db466efe86cd6dc5a"
     ]
    },
    "id": "15",
    "outputId": "406cc857-cf73-49eb-c3da-2c62ddc36f69"
   },
   "outputs": [],
   "source": [
    "dpo_df = pd.DataFrame(pairs, columns=[\"prompt\", \"chosen\", \"rejected\"])\n",
    "dpo_ds = Dataset.from_pandas(dpo_df)\n",
    "dpo_ds.save_to_disk(str(DPO_DATA))\n",
    "dpo_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# _reload = load_from_disk(str(DPO_DATA))\n",
    "# print(_reload, _reload[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "am4jR7d4-Qul"
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "17"
   },
   "outputs": [],
   "source": [
    "policy_model.config.use_cache = False\n",
    "policy_model.train()\n",
    "\n",
    "ref_model = copy.deepcopy(policy_model)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "ref_model.eval()\n",
    "ref_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "3d7e12dc6f6840c4b3445e3d7786e972",
      "9c0504d55afe405388b1511ffe3aed9b",
      "aa937f7a7bfd49b6955c1f74fd349767",
      "47a21abd9bdc485594ab0051c07080c4",
      "35b008c0ef394a109432ff950de0684e",
      "9967ecdeaa6146be81eea2cefdecafc8",
      "c3bbf73af92545afb8f1a0e32c3bc7a9",
      "9b5ef28023514901a4bb144d74d28847",
      "1cb454671c434ebe90f51839178a7913",
      "5fb7469f77794d3fb0cba76d80aa9ef6",
      "b06e071fc064444ca8d1be9526b8cbda",
      "30e8542c2cff4291b9487a48d45326f8",
      "9dc41d10fbc741459f738d7b7c707e88",
      "27e1ac64d1a14a9baf4e21cadd41fa9f",
      "96f213ff5bdb4f3c8135bfe232ded988",
      "e91166d11cc04f668936ee8c9d24014d",
      "8dd5ba09447846688f6ff2f0a3ce5347",
      "c61905e604974bc2b0053f6eab2d6968",
      "0041d9b5f7ea48f9831314dcd44813ee",
      "e8572ea938c84320947cfb3ee133aceb",
      "8db212aeee8842889726cad53fc1bb87",
      "e750612e78f14ebf81ba481788b7d070",
      "d9f5320d8910411da9485a4a3a09058e",
      "71d438d7ab05452fa2d8303a44456530",
      "f5b76d57a62e48b18c3f38282e34e02f",
      "bc7f8929b66f4c918d2d228465ab4c49",
      "a20278578dae457fa002436d959946d4",
      "96d90a7b6b0c4b5ab6d4c4787c65b13b",
      "ed4d8014fb1049018df35e3d9aa83f03",
      "767fdd09906e4e179100905f858559e3",
      "0eedd07f76fa41b997b4dfc058bdd97c",
      "2f9b7f8bdbb94e40a52fe5fad0d57625",
      "56e57d04d77a48a3b106380f1472e80c"
     ]
    },
    "id": "18",
    "outputId": "582fd6f4-7dd7-4368-e9c7-1b0f4faa80a9"
   },
   "outputs": [],
   "source": [
    "check_dir = ROOT / \"models/dpo_qlora\"\n",
    "dpo_cfg = DPOConfig(\n",
    "    output_dir=str(check_dir),\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    beta=0.1,\n",
    "    max_prompt_length=2048,\n",
    "    max_length=2048 + 256,\n",
    "    bf16=True,\n",
    "    optim=(\n",
    "        \"adamw_bnb_8bit\"\n",
    "        if use_4bit\n",
    "        else (\"adamw_torch_fused\" if device == \"cuda\" else \"adamw_torch\")\n",
    "    ),\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_cfg,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dpo_ds,\n",
    ")\n",
    "\n",
    "print(\"active adapters:\", getattr(dpo_trainer.model, \"active_adapters\", None))\n",
    "dpo_trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "19",
    "outputId": "9e562ba5-b171-49c1-d694-f134e322321f"
   },
   "outputs": [],
   "source": [
    "train_out = dpo_trainer.train()\n",
    "train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20",
    "outputId": "abeaec7a-8bee-4954-b08c-f2832a5b10ab"
   },
   "outputs": [],
   "source": [
    "out_dir = ROOT / \"models/dpo_qlora\"\n",
    "policy_model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "21",
    "outputId": "2adc02f6-116f-4607-e93f-f64e86613f63"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = \"/content/dpo_qwen2_qlora_adapter\"\n",
    "shutil.make_archive(zip_path, \"zip\", out_dir)\n",
    "files.download(zip_path + \".zip\")\n",
    "\n",
    "!cp -r {Path(\"/content/llm-news/src/checkpoints/dpo_qlora\")} \"/content/drive/MyDrive/llm-news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "3T7E1NUJ-xCy"
   },
   "outputs": [],
   "source": [
    "del policy_model, ref_model, base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c2bc63f5dc6641b881f8ced4a1130b2c",
      "ea42a8b4ac244c76979c341f54e73d26",
      "eb2064b215314ba39c8da21e9bb55550",
      "52ca8dc961b14b81872d96746475d53b",
      "524fa9e976ca458bb93f27bf62035c7e",
      "c4d8856cee924daeb8a55e8c0367c10b",
      "2f82189469dd4e4097435f5ceaa7e8b2",
      "fdca7195b8cf4836b5ce337e9f08dbe3",
      "ee6ad72400ed4ad496313aa6e6d0d638",
      "8d54803f9fd547568c8470a34b33a83b",
      "45594fbc2c2f4e9f8e786ee71ab15e9a"
     ]
    },
    "id": "22",
    "outputId": "93451ac7-8126-49bd-e1aa-09fced105021"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "policy_model = PeftModel.from_pretrained(base_model, out_dir)\n",
    "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(policy_model, \"generation_config\", None) is not None:\n",
    "    policy_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    policy_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "policy_model.eval()\n",
    "policy_model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "84ed62863d7d47adae4b45e04b5357fb",
      "c6a6f1e9b23544c4809fff69f84b101c",
      "0da1a0de170f46b2b26b9c13f24bfb84",
      "f2f61b177d124ae39d30f2279f6408f2",
      "9c9d1ed02bbf4259824999d6702a0fa1",
      "4700b8f577184c8c92b9d84c04ed7f42",
      "0b25a92e15c14cdaa5e21fd6412400fb",
      "58a0e66de31c4100a0d5b6af57089d14",
      "a0aca17994c144bda5b2c5ecf97d036e",
      "7e85b7300bd54bc887103364eae54cb1",
      "c3b8a995042b49d392e0fe6a6c2ef97f"
     ]
    },
    "id": "23",
    "outputId": "77a94f6a-94ae-4909-f88f-0af90ec18caa"
   },
   "outputs": [],
   "source": [
    "BATCH = 6\n",
    "texts = subset_val[\"text\"].tolist()\n",
    "refs = subset_val[\"summary\"].tolist()\n",
    "\n",
    "preds_dpo = generate_with_cfg(texts, policy_model, GEN_GREEDY, batch_size=BATCH)\n",
    "\n",
    "\n",
    "scores_dpo = data_utils.get_rouge_f1(preds_dpo, refs)\n",
    "scores_dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24",
    "outputId": "11f38364-36de-45c7-dc75-dec338944898"
   },
   "outputs": [],
   "source": [
    "preds_dpo[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25",
    "outputId": "a0eebbf2-2007-42d7-8b38-1fe23acfdebd"
   },
   "outputs": [],
   "source": [
    "refs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "fd5eafe559634b40b95518d10bb70944",
      "c6f515337e21467891951d32bd55d09c",
      "e34bd1df18394e5bb01b39e411737121",
      "e6f6351bac2d4458ab292857e0fc202d",
      "423a9291fa9d4388928dfa56caca7855",
      "1314900d14924ee7a22b9e09f2ca742b",
      "34271590a7b2413c8f1e798822951ece",
      "aafcc563a0f9467b930b8306fcd8c69b",
      "32bc92461fac45bc8de2cbe75108d7b0",
      "8131c68feb594910ad4e70cdb358d40b",
      "1757649dde0d4d75b629e0d48dbeb8df",
      "df2030316b3448a1b07be74091470b75",
      "71163c617caf4bfb86c523b60c0a6e62",
      "c15ec7898bd64e60a03c3afa473a6741",
      "6c2048acc5c541a8b3d90588544afcde",
      "785abe5b4c3d459ea6af9e7db2695419",
      "dd94880794654341a5308f27bb0594ae",
      "55cfea360eb64a47ac0a6f5ed1c39776",
      "3f4bfa95cb0644e49a07c560854f3f36",
      "7bfbb79a732040fca1c185d91f38373a",
      "0e32aabfe6364a02bb024410f164e004",
      "fca0151d27204bc6b602b7a10d510264",
      "7eb6b484492641f9adde3ea4f5402fd9",
      "0f0a6edad2314602a29dd94ae07caada",
      "0a0e4f7bb41549f682c1780b55c4a15b",
      "eaa3c0bb7f6847d5b0cfb8bff7f81a7e",
      "99b84e6342b942898a6811d157d4debf",
      "7419ecc72e744b57bd7278c848c9e536",
      "550cc1e37ef54609a2c9f72afac176d3",
      "7b804babecbb4c38bc74b657c58f67a8",
      "e35ec9c016d44a16b96da63a9c8abd03",
      "5940e9d58caa413a9fde9d03014d8924",
      "2b9a741ace414974ab18d1f6040c5556",
      "ce8448f597d640c39029638b2877cd70",
      "d24cc661ecfd429cadd15f8b391dbe05",
      "b725295131f3468281a9ea97c6a1793e",
      "24fa7946c76947ff90632e40142bdd6a",
      "10a8572eda5349e6a63d336ff81d986c",
      "f7dc4ca9cc4c496ea657c45f6a8d615d",
      "d8e6ac40faf64161a9e4e16295f17215",
      "36648c2f0d214e5e826403b6deffdf47",
      "9e7fd967d1644302bc19df26b5bfc1f8",
      "1f884e8e80534be29c17e1963c045e81",
      "a0903b7be95c425ca732c1600ee5fd46",
      "ce0daeb8e5a24f0a9d2505bc84bf0aba",
      "a1a7c992bf68486f95e3f9a547007f54",
      "eb9dbd28f7384b04b9ba1260b9374f9f",
      "45d2617b78b746148e5a0be2c4f6dc84",
      "f5c85c820f7949f897da1ee260bc2191",
      "880499dea30445e9b5e78780ad90cdd0",
      "0fe9c91f1b084a59bad097266884ae1f",
      "db3b75cb9c554c5c86e2ed99947905ad",
      "be82272e9a4648bc897d10c5f785c56f",
      "2bd91a160029460bb3b804d50205de67",
      "59fd0db6ab03480f98df861c0d8056b1",
      "7a22cbcc377c4b60a0954f577580cf1d",
      "58af5d5975384bf193ca4cc9d2c1ad84",
      "e40dab65d2374ac482ae0332dce4392e",
      "d610c567829b4037b9bde01f6d7f3b4b",
      "4c409e2a40e749579302c31869515b18",
      "e4b2917e8be34a63a3cde54a3be64cb0",
      "4f54a17cc54c44b597a9332208f1adc0",
      "20fe6021836041b38fc08a4e73d27585",
      "ef4014da3f58405d87c13aff3d5ee384",
      "4f06ba5c627146a8871273e2be54914a",
      "9cbf3f069af04ce6baf5f1e9466a109e"
     ]
    },
    "id": "26",
    "outputId": "ba3396b8-569f-4917-aa54-f4c2a791451b"
   },
   "outputs": [],
   "source": [
    "scores = data_utils.get_all_scores(preds_dpo, refs, device=device)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "27"
   },
   "outputs": [],
   "source": [
    "Path(METRIC_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"system\": \"SRT+DPO QLoRA\",\n",
    "            \"split\": \"validation_full\",\n",
    "            \"rouge1\": scores.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": scores.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": scores.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": scores.get(\"rougeLsum\", 0.0),\n",
    "            \"bertscore_precision\": scores.get(\"bertscore_precision\", 0.0),\n",
    "            \"bertscore_recall\": scores.get(\"bertscore_recall\", 0.0),\n",
    "            \"bertscore_f1\": scores.get(\"bertscore_f1\", 0.0),\n",
    "            \"avg_len_pred\": scores.get(\"avg_len_pred\", 0.0),\n",
    "            \"avg_len_ref\": scores.get(\"avg_len_ref\", 0.0),\n",
    "            \"len_ratio_pred_to_ref\": scores.get(\"len_ratio_pred_to_ref\", 0.0),\n",
    "            \"k\": None,\n",
    "            \"n_examples\": N_EVAL,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_metrics.to_csv(\n",
    "    METRIC_DIR / f\"llm_dpo_qlora_validation_{device}_{N_EVAL}.csv\", index=False\n",
    ")\n",
    "\n",
    "df_sampels = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\": subset_val[\"title\"].head(3) if \"title\" in subset_val else [\"\"] * 3,\n",
    "            \"reference\": refs[:3],\n",
    "            \"prediction\": preds_dpo[:3],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df_sampels.to_csv(\n",
    "    METRIC_DIR / f\"llm_dpo_qlora_examples_{device}.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28",
    "outputId": "b3ad4b93-45a3-417c-c9d6-3c19d0135b9f"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| CUDA доступна:\", torch.cuda.is_available())\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# REPO_URL = \"https://github.com/mdayssi/llm-news-summarizer-ru.git\"\n",
    "# REPO_DIR = \"/content/llm-news\"\n",
    "\n",
    "# if not os.path.exists(REPO_DIR):\n",
    "#     !git clone {REPO_URL} {REPO_DIR}\n",
    "# else:\n",
    "#     print(\"Репозиторий уже есть:\", REPO_DIR)\n",
    "\n",
    "\n",
    "# %cd {REPO_DIR}\n",
    "# !git rev-parse --short HEAD\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# %pip -q install --upgrade \\\n",
    "#   evaluate rouge-score bert_score\\\n",
    "#   razdel bitsandbytes accelerate\\\n",
    "#   python-dotenv pyyaml peft trl\n",
    "\n",
    "# import accelerate\n",
    "# import bert_score\n",
    "# import bitsandbytes\n",
    "# import datasets\n",
    "# import dotenv\n",
    "# import evaluate\n",
    "# import razdel\n",
    "# import rouge_score\n",
    "# import sentencepiece\n",
    "# import torch\n",
    "# import tqdm\n",
    "# import transformers\n",
    "# import yaml\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| cuda avail:\", torch.cuda.is_available())\n",
    "# print(\"transformers:\", transformers.__version__)\n",
    "# print(\"datasets:\", datasets.__version__)\n",
    "# print(\"evaluate:\", evaluate.__version__)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# repo_src = \"/content/llm-news/src\"\n",
    "# if repo_src not in sys.path:\n",
    "#     sys.path.insert(0, repo_src)\n",
    "# print(\"sys.path ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "29"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
