{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from src.utils import data as data_utils\n",
    "from src.utils import io as io_utils\n",
    "from src.utils import models as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# EXTERNAL = Path(os.getenv(\"EXTERNAL_STORAGE_DIR\"))\n",
    "ROOT = io_utils.repo_root()\n",
    "CONFIG_DIR = ROOT / \"config\"\n",
    "METRIC_DIR = ROOT / \"metrics\"\n",
    "SFT_MODEL_DIR = ROOT / \"models/sft_qlora\"\n",
    "DPO_MODEL_DIR = ROOT / \"models/dpo_qlora\"\n",
    "RANDOM_STATE = 42\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2",
    "outputId": "392b91fd-4602-4572-8b3c-04f9fabf2f88"
   },
   "outputs": [],
   "source": [
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CFG_PATH = CONFIG_DIR / \"models.params.yml\"\n",
    "model_cfg = None\n",
    "if use_cuda:\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cuda_model\"]\n",
    "else:\n",
    "    model_cfg = io_utils.load_yaml(MODEL_CFG_PATH)[\"cpu_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7",
    "outputId": "56cee17b-4ee7-41aa-e46f-77777cb521ab"
   },
   "outputs": [],
   "source": [
    "model_id = model_cfg[\"model_id\"]\n",
    "use_4bit = model_cfg[\"use_4bit\"]\n",
    "device_map = model_cfg[\"device_map\"]\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if use_cuda and torch.cuda.is_bf16_supported()\n",
    "    else (torch.float16 if use_cuda else torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "quantization_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes не готов, продолжаем без 4-бит:\", e)\n",
    "        quantization_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loaded = {\"which\": None, \"base\": None, \"policy\": None}\n",
    "\n",
    "\n",
    "def _unload_current():\n",
    "    if _loaded[\"policy\"] is not None:\n",
    "        try:\n",
    "            del _loaded[\"policy\"], _loaded[\"base\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        if use_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "    _loaded.update({\"which\": None, \"base\": None, \"policy\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(which: str):\n",
    "    assert which in {\"SFT\", \"DPO\"}, \"which должен быть 'SFT' или 'DPO'\"\n",
    "\n",
    "    if _loaded[\"which\"] == which and _loaded[\"policy\"] is not None:\n",
    "        return _loaded[\"policy\"]\n",
    "\n",
    "    _unload_current()\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "\n",
    "    adapter_dir = SFT_MODEL_DIR if which == \"SFT\" else DPO_MODEL_DIR\n",
    "    policy = PeftModel.from_pretrained(base, adapter_dir)\n",
    "    policy.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if getattr(policy, \"generation_config\", None) is not None:\n",
    "        policy.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        policy.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    policy.eval()\n",
    "    policy.config.use_cache = True\n",
    "\n",
    "    _loaded.update({\"which\": which, \"base\": base, \"policy\": policy})\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    m = load_adapter(\"DPO\")\n",
    "except AssertionError:\n",
    "    m = load_adapter(\"SFT\")\n",
    "\n",
    "device = next(m.parameters()).device\n",
    "print(\"device: \", device, \"adapter: \", _loaded[\"which\"])\n",
    "\n",
    "_unload_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Ты помощник по резюмированию русскоязычных новостей. \"\n",
    "    \"Сделай краткое, нейтральное резюме исходного текста (3–5 предложений). \"\n",
    "    \"Не добавляй фактов, которых нет в тексте.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_chat(text: str) -> str:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Задача: кратко резюмируй.\\n\\nТекст статьи:\\n{text}\",\n",
    "        },\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with(\n",
    "    model, text, max_new_tokens=200, do_sample=False, temperature=0.7, top_p=0.9\n",
    "):\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        do_sample=bool(do_sample),\n",
    "        temperature=float(temperature),\n",
    "        top_p=float(top_p),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    prompt = build_chat(text)\n",
    "\n",
    "    max_inp = model_utils.get_max_input_tokens(tokenizer, gen_cfg)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        pad_to_multiple_of=8,\n",
    "        max_length=max_inp,\n",
    "    )\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=gen_cfg,\n",
    "        )\n",
    "\n",
    "    gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1] :]\n",
    "    text_out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return text_out.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ui_summarize(\n",
    "    article, system_choice, max_new_tokens, do_sample, temperature, top_p, reference\n",
    "):\n",
    "    t0 = time.time()\n",
    "\n",
    "    if not article or not article.strip():\n",
    "        return \"\", \"\", \"0.00 s\", 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    if system_choice == \"Lead-3\":\n",
    "        pred = model_utils.lead3(article)\n",
    "    else:\n",
    "        which = \"SFT\" if system_choice == \"SFT QLoRA\" else \"DPO\"\n",
    "        model = load_adapter(which)\n",
    "        pred = summarize_with(\n",
    "            model,\n",
    "            article,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "    dt = f\"{time.time()-t0:.2f} s\"\n",
    "\n",
    "    # опционально считаем ROUGE, если пользователь дал reference\n",
    "    r1 = r2 = rL = rLsum = 0.0\n",
    "    if reference and reference.strip():\n",
    "        score = data_utils.get_rouge_f1([pred], [reference])\n",
    "        r1, r2, rL, rLsum = (\n",
    "            score[\"rouge1\"],\n",
    "            score[\"rouge2\"],\n",
    "            score[\"rougeL\"],\n",
    "            score[\"rougeLsum\"],\n",
    "        )\n",
    "\n",
    "    return pred, (reference or \"\"), dt, r1, r2, rL, rLsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    gr.Markdown(\"### Мини-интерфейс суммаризации новостей (Lead-3 / SFT / SFT+DPO)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            article = gr.Textbox(\n",
    "                label=\"Текст статьи\", lines=18, placeholder=\"Вставьте текст новости…\"\n",
    "            )\n",
    "            reference = gr.Textbox(label=\"(Опционально) Эталон для ROUGE\", lines=4)\n",
    "        with gr.Column(scale=1):\n",
    "            system_choice = gr.Radio(\n",
    "                choices=[\"Lead-3\", \"SFT QLoRA\", \"SFT+DPO QLoRA\"],\n",
    "                value=\"SFT+DPO QLoRA\",\n",
    "                label=\"Система\",\n",
    "            )\n",
    "            max_new = gr.Slider(64, 512, value=200, step=8, label=\"max_new_tokens\")\n",
    "            do_sample = gr.Checkbox(value=False, label=\"do_sample\")\n",
    "            temperature = gr.Slider(0.1, 1.5, value=0.7, step=0.05, label=\"temperature\")\n",
    "            top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"top_p\")\n",
    "            btn = gr.Button(\"Суммаризировать\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            pred = gr.Textbox(label=\"Результат\", lines=12)\n",
    "            tspent = gr.Textbox(label=\"Время\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"**ROUGE (если указан эталон)**\")\n",
    "            r1 = gr.Number(label=\"rouge1\")\n",
    "            r2 = gr.Number(label=\"rouge2\")\n",
    "            rL = gr.Number(label=\"rougeL\")\n",
    "            rLsum = gr.Number(label=\"rougeLsum\")\n",
    "\n",
    "    btn.click(\n",
    "        ui_summarize,\n",
    "        inputs=[\n",
    "            article,\n",
    "            system_choice,\n",
    "            max_new,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            reference,\n",
    "        ],\n",
    "        outputs=[pred, reference, tspent, r1, r2, rL, rLsum],\n",
    "    )\n",
    "\n",
    "# demo.queue()\n",
    "demo.launch(debug=False, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()\n",
    "_unload_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28",
    "outputId": "b3ad4b93-45a3-417c-c9d6-3c19d0135b9f"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| CUDA доступна:\", torch.cuda.is_available())\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# REPO_URL = \"https://github.com/mdayssi/llm-news-summarizer-ru.git\"\n",
    "# REPO_DIR = \"/content/llm-news\"\n",
    "\n",
    "# if not os.path.exists(REPO_DIR):\n",
    "#     !git clone {REPO_URL} {REPO_DIR}\n",
    "# else:\n",
    "#     print(\"Репозиторий уже есть:\", REPO_DIR)\n",
    "\n",
    "\n",
    "# %cd {REPO_DIR}\n",
    "# !git rev-parse --short HEAD\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "# %pip -q install --upgrade \\\n",
    "#   evaluate rouge-score bert_score\\\n",
    "#   razdel bitsandbytes accelerate\\\n",
    "#   python-dotenv pyyaml peft trl\n",
    "\n",
    "# import accelerate\n",
    "# import bert_score\n",
    "# import bitsandbytes\n",
    "# import datasets\n",
    "# import dotenv\n",
    "# import evaluate\n",
    "# import razdel\n",
    "# import rouge_score\n",
    "# import sentencepiece\n",
    "# import torch\n",
    "# import tqdm\n",
    "# import transformers\n",
    "# import yaml\n",
    "\n",
    "# print(\"torch:\", torch.__version__, \"| cuda avail:\", torch.cuda.is_available())\n",
    "# print(\"transformers:\", transformers.__version__)\n",
    "# print(\"datasets:\", datasets.__version__)\n",
    "# print(\"evaluate:\", evaluate.__version__)\n",
    "\n",
    "# # ----------------------------------------------------------------------------------\n",
    "\n",
    "# repo_src = \"/content/llm-news/src\"\n",
    "# if repo_src not in sys.path:\n",
    "#     sys.path.insert(0, repo_src)\n",
    "# print(\"sys.path ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "29"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (llm-news)",
   "language": "python",
   "name": "llm-news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
